# -*- coding: utf-8 -*-
"""Logisoft_Hackathon_RL_Optimizer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Zd58HLsVQqOJdMTyVtlr-k5tXSyqvlhZ
"""

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from matplotlib.animation import FuncAnimation
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from collections import deque, namedtuple
import random
import pickle
import json
from datetime import datetime
import os

# ================================
# 1. DATASET GENERATION
# ================================

class VehicleDatasetGenerator:
    """Generate synthetic vehicle placement scenarios for training"""

    def __init__(self, yard_width=20, yard_height=15):
        self.yard_width = yard_width
        self.yard_height = yard_height
        self.vehicle_types = {
            'car': {'width': 2, 'height': 4, 'priority': 1},
            'suv': {'width': 2, 'height': 5, 'priority': 1},
            'truck': {'width': 3, 'height': 8, 'priority': 2},
            'bus': {'width': 3, 'height': 10, 'priority': 3},
            'van': {'width': 2, 'height': 6, 'priority': 1}
        }

    def generate_scenario(self, num_vehicles=50):
        """Generate a single placement scenario"""
        vehicles = []
        for i in range(num_vehicles):
            vehicle_type = random.choice(list(self.vehicle_types.keys()))
            arrival_time = random.randint(0, 100)
            duration = random.randint(30, 300)  # Stay duration

            vehicles.append({
                'id': i,
                'type': vehicle_type,
                'width': self.vehicle_types[vehicle_type]['width'],
                'height': self.vehicle_types[vehicle_type]['height'],
                'priority': self.vehicle_types[vehicle_type]['priority'],
                'arrival_time': arrival_time,
                'duration': duration
            })

        return sorted(vehicles, key=lambda x: x['arrival_time'])

    def generate_dataset(self, num_scenarios=1000):
        """Generate multiple scenarios for training"""
        dataset = []
        for i in range(num_scenarios):
            scenario = self.generate_scenario()
            dataset.append({
                'scenario_id': i,
                'vehicles': scenario,
                'yard_size': (self.yard_width, self.yard_height)
            })
        return dataset

    def save_dataset(self, dataset, filename='vehicle_dataset.pkl'):
        """Save dataset to file"""
        with open(filename, 'wb') as f:
            pickle.dump(dataset, f)
        print(f"Dataset saved to {filename}")

# ================================
# 2. ENVIRONMENT
# ================================

class VehicleYardEnvironment:
    """Vehicle placement environment for RL training"""

    def __init__(self, width=20, height=15, max_steps=200):
        self.width = width
        self.height = height
        self.max_steps = max_steps
        self.zone_map = self._generate_zone_map()
        self.reset()

    def _generate_zone_map(self):
        zone_map = np.ones((self.height, self.width), dtype=int)

        # Define road width and spacing
        road_width = 1  # Width of roads
        parking_space_width = 6  # Width of parking spaces
        parking_space_height = 19  # Height of parking spaces

        # Create horizontal roads
        for y in range(0, self.height, road_width + parking_space_height):
            zone_map[y:y + road_width, :] = 0  # Create horizontal road

        # Create vertical roads
        for x in range(0, self.width, road_width + parking_space_width):
            zone_map[:, x:x + road_width] = 0  # Create vertical road

        return zone_map

    def reset(self):
        """Reset environment to initial state"""
        self.grid = np.zeros((self.height, self.width), dtype=int)
        self.placed_vehicles = []
        self.current_vehicle = None
        self.vehicle_queue = []
        self.step_count = 0
        self.total_reward = 0
        self.zone_map = self._generate_zone_map()
        self.occupancy_rate = 0

        # Generate random vehicle sequence
        generator = VehicleDatasetGenerator(self.width, self.height)
        vehicles = generator.generate_scenario(30)
        self.vehicle_queue = vehicles.copy()

        if self.vehicle_queue:
            self.current_vehicle = self.vehicle_queue.pop(0)

        return self._get_state()

    def _get_state(self):
        """Get current state representation"""
        # State includes: grid, current vehicle info, queue info
        state = {
            'grid': self.grid.copy(),
            'current_vehicle': self.current_vehicle,
            'queue_length': len(self.vehicle_queue),
            'occupancy_rate': self.occupancy_rate,
            'zone_map': self.zone_map.copy(),
            'step_count': self.step_count
        }
        return state

    def _can_place_vehicle(self, x, y, vehicle):
        """Check if vehicle can be placed at position (x, y)"""
        if x + vehicle['width'] > self.width or y + vehicle['height'] > self.height:
            return False

        for dy in range(vehicle['height']):
            for dx in range(vehicle['width']):
                if self.grid[y + dy, x + dx] != 0 or self.zone_map[y + dy, x + dx] != 1:
                    return False

        return True

    def _place_vehicle(self, x, y, vehicle):
        """Place vehicle at position (x, y)"""
        vehicle_id = vehicle['id']
        for dy in range(vehicle['height']):
            for dx in range(vehicle['width']):
                self.grid[y + dy, x + dx] = vehicle_id + 1  # +1 to avoid 0

        self.placed_vehicles.append({
            **vehicle,
            'x': x, 'y': y,
            'placement_step': self.step_count
        })

    def step(self, action):
        """Execute action and return next state, reward, done"""
        reward = 0
        done = False

        if self.current_vehicle is None:
            return self._get_state(), 0, True, {}

        # Action is (x, y) position or skip action
        if action == 'skip':
            # Skip current vehicle (penalty)
            reward = -10
        else:
            x, y = action
            if not self._can_place_vehicle(x, y, self.current_vehicle):
                # Attempted to place on a road or invalid spot
                reward = -30  # Heavier penalty for ignoring road rules
            else:
                # if self._can_place_vehicle(x, y, self.current_vehicle):
                self._place_vehicle(x, y, self.current_vehicle)

                # Calculate reward based on placement efficiency
                total_parkable_blocks = np.sum(self.zone_map == 1)
                occupied_blocks = np.sum((self.grid > 0) & (self.zone_map == 1))
                space_utilization = occupied_blocks / total_parkable_blocks if total_parkable_blocks > 0 else 0
                compactness_reward = self._calculate_compactness_reward()
                priority_reward = self.current_vehicle['priority'] * 2

                reward = 10 + compactness_reward + priority_reward

                # Bonus for short stays
                if self.current_vehicle['duration'] < 100:
                    reward += 5

                # Penalty if high-priority vehicle is skipped or delayed
                if action == 'skip' and self.current_vehicle['priority'] == 3:
                    reward -= 20

                if self.zone_map[y, x] == 1:
                    reward += 2  # Bonus for compliant parking
                else:
                    reward -= 10  # Redundant but safe

                self.occupancy_rate = space_utilization
            # else:
            #     reward = -20

        # Move to next vehicle
        if self.vehicle_queue:
            self.current_vehicle = self.vehicle_queue.pop(0)
        else:
            self.current_vehicle = None
            done = True

        self.step_count += 1
        self.total_reward += reward

        if self.step_count >= self.max_steps:
            done = True

        info = {
            'occupancy_rate': self.occupancy_rate,
            'vehicles_placed': len(self.placed_vehicles),
            'total_reward': self.total_reward
        }

        return self._get_state(), reward, done, info

    def _calculate_compactness_reward(self):
        """Calculate reward based on how compact the placement is"""
        if len(self.placed_vehicles) < 2:
            return 0

        # Reward for placing vehicles close to each other
        last_vehicle = self.placed_vehicles[-1]
        compactness = 0

        for vehicle in self.placed_vehicles[:-1]:
            distance = abs(vehicle['x'] - last_vehicle['x']) + abs(vehicle['y'] - last_vehicle['y'])
            compactness += max(0, 10 - distance)  # Closer = better

        return compactness / len(self.placed_vehicles)

    def get_valid_actions(self):
        """Get all valid placement positions for current vehicle"""
        if self.current_vehicle is None:
            return []

        valid_actions = []
        for y in range(self.height - self.current_vehicle['height'] + 1):
            for x in range(self.width - self.current_vehicle['width'] + 1):
                if self._can_place_vehicle(x, y, self.current_vehicle):
                    valid_actions.append((x, y))

        return valid_actions

# ================================
# 3. DQN MODEL
# ================================

class DQN(nn.Module):
    """Deep Q-Network for vehicle placement"""

    def __init__(self, state_size, action_size, hidden_size=256):
        super(DQN, self).__init__()

        # Convolutional layers for grid processing
        self.conv1 = nn.Conv2d(2, 32, kernel_size=3, padding=1)  # changed from 1 to 2 channels
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, padding=1)

        # Calculate conv output size
        conv_out_size = 64 * 15 * 20  # Assuming 15x20 grid

        # Vehicle feature processing
        self.vehicle_fc = nn.Linear(5, 32)  # vehicle features

        # Combined processing
        self.fc1 = nn.Linear(conv_out_size + 32 + 3, hidden_size)  # +3 for extra features
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, action_size)

        self.dropout = nn.Dropout(0.2)

    def forward(self, state_dict):
        # Process grid with CNN
        grid = state_dict['grid'].float()         # [B, 1, H, W]
        zone = state_dict['zone_map'].float()     # [B, 1, H, W]
        x_input = torch.cat([grid, zone], dim=1)  # [B, 2, H, W]
        x_grid = F.relu(self.conv1(x_input))
        x_grid = F.relu(self.conv2(x_grid))
        x_grid = F.relu(self.conv3(x_grid))
        x_grid = x_grid.view(x_grid.size(0), -1)  # Flatten

        # Process vehicle features
        vehicle = state_dict['vehicle_features'].float()
        x_vehicle = F.relu(self.vehicle_fc(vehicle))

        # Process additional features
        extra_features = state_dict['extra_features'].float()

        # Combine all features
        x = torch.cat([x_grid, x_vehicle, extra_features], dim=1)

        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = F.relu(self.fc2(x))
        x = self.fc3(x)

        return x

# ================================
# 4. RL AGENT
# ================================

Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])

class DQNAgent:
    """DQN Agent for vehicle placement optimization"""

    def __init__(self, state_size, action_size, lr=0.001, epsilon=1.0, epsilon_decay=0.995):
        self.state_size = state_size
        self.action_size = action_size
        self.grid_width = 20
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = 0.01

        # Neural networks
        self.q_network = DQN(state_size, action_size)
        self.target_network = DQN(state_size, action_size)
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)

        # Experience replay
        self.memory = deque(maxlen=10000)
        self.batch_size = 32

        # Training parameters
        self.gamma = 0.95
        self.tau = 0.001  # For soft update
        self.update_every = 4
        self.step_count = 0

    def _state_to_tensor(self, state):
        """Convert state to tensor format for neural network"""
        if state['current_vehicle'] is None:
            vehicle_features = torch.zeros(5)
        else:
            v = state['current_vehicle']
            vehicle_features = torch.tensor([
                v['width'], v['height'], v['priority'],
                v.get('arrival_time', 0), v.get('duration', 0)
            ])

        extra_features = torch.tensor([
            state['queue_length'],
            state['occupancy_rate'],
            state['step_count'] / 200  # Normalized
        ])

        return {
            'grid': torch.tensor(state['grid'], dtype=torch.float).unsqueeze(0).unsqueeze(0),
            'zone_map': torch.tensor(state['zone_map'], dtype=torch.float).unsqueeze(0).unsqueeze(0),
            'vehicle_features': vehicle_features.unsqueeze(0),
            'extra_features': extra_features.unsqueeze(0)
        }

    def act(self, state, valid_actions):
      """Choose action using epsilon-greedy policy"""
      if not valid_actions:
          return 'skip'

      if random.random() <= self.epsilon:
          return random.choice(valid_actions)

      state_tensor = self._state_to_tensor(state)
      q_values = self.q_network(state_tensor)

      action_values = []
      for action in valid_actions:
          if isinstance(action, tuple):
              x, y = action
              action_idx = y * self.grid_width + x
              action_values.append((q_values[0, action_idx].item(), action))

      if action_values:
          return max(action_values, key=lambda x: x[0])[1]
      return random.choice(valid_actions)

    def remember(self, state, action, reward, next_state, done):
        """Store experience in replay buffer"""
        self.memory.append(Experience(state, action, reward, next_state, done))

    def replay(self):
        """Train the model on a batch of experiences"""
        if len(self.memory) < self.batch_size:
            return

        experiences = random.sample(self.memory, self.batch_size)
        self._learn(experiences)

        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

    def _learn(self, experiences):
      states, actions, rewards, next_states, dones = zip(*experiences)

      state_tensors = [self._state_to_tensor(s) for s in states]
      next_state_tensors = [self._state_to_tensor(s) for s in next_states]

      # Stack into batches
      batch_grid = torch.cat([s['grid'] for s in state_tensors])
      batch_vehicle = torch.cat([s['vehicle_features'] for s in state_tensors])
      batch_extra = torch.cat([s['extra_features'] for s in state_tensors])
      batch_zone = torch.cat([s['zone_map'] for s in state_tensors])
      batch_next_zone = torch.cat([s['zone_map'] for s in next_state_tensors])
      batch_next_grid = torch.cat([s['grid'] for s in next_state_tensors])
      batch_next_vehicle = torch.cat([s['vehicle_features'] for s in next_state_tensors])
      batch_next_extra = torch.cat([s['extra_features'] for s in next_state_tensors])

      q_values = self.q_network({
        'grid': batch_grid,
        'zone_map': batch_zone,
        'vehicle_features': batch_vehicle,
        'extra_features': batch_extra
    })

      with torch.no_grad():
          next_q_values = self.target_network({
            'grid': batch_next_grid,
            'zone_map': batch_next_zone,
            'vehicle_features': batch_next_vehicle,
            'extra_features': batch_next_extra
        })

      target_q_values = q_values.clone().detach()

      for i in range(len(experiences)):
          action = actions[i]
          if action == 'skip':
              action_idx = self.action_size - 1  # Reserved index
          else:
              x, y = action
              action_idx = y * self.grid_width + x

          target = rewards[i]
          if not dones[i]:
              target += self.gamma * torch.max(next_q_values[i]).item()

          target_q_values[i][action_idx] = target

      loss = F.mse_loss(q_values, target_q_values)

      self.optimizer.zero_grad()
      loss.backward()
      self.optimizer.step()

    def save_model(self, filepath):
        """Save trained model"""
        torch.save({
            'q_network_state_dict': self.q_network.state_dict(),
            'target_network_state_dict': self.target_network.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'epsilon': self.epsilon
        }, filepath)

    def load_model(self, filepath):
        """Load trained model"""
        checkpoint = torch.load(filepath)
        self.q_network.load_state_dict(checkpoint['q_network_state_dict'])
        self.target_network.load_state_dict(checkpoint['target_network_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.epsilon = checkpoint['epsilon']

# ================================
# 5. TRAINING SYSTEM
# ================================

class VehiclePlacementTrainer:
    """Training system for the RL agent"""

    def __init__(self):
        self.env = VehicleYardEnvironment()
        self.agent = DQNAgent(state_size=1000, action_size=300)  # Simplified sizes
        self.training_history = []

    def train(self, episodes=1000):
        """Train the agent"""
        print("Starting training...")

        for episode in range(episodes):
            state = self.env.reset()
            total_reward = 0
            steps = 0

            while True:
                valid_actions = self.env.get_valid_actions()
                if not valid_actions and state['current_vehicle'] is None:
                    break

                action = self.agent.act(state, valid_actions + ['skip'])
                next_state, reward, done, info = self.env.step(action)

                self.agent.remember(state, action, reward, next_state, done)
                state = next_state
                total_reward += reward
                steps += 1

                if done:
                    break

            # Train the agent
            self.agent.replay()

            # Log progress
            self.training_history.append({
                'episode': episode,
                'total_reward': total_reward,
                'steps': steps,
                'occupancy_rate': info.get('occupancy_rate', 0),
                'vehicles_placed': info.get('vehicles_placed', 0),
                'epsilon': self.agent.epsilon
            })

            if episode % 100 == 0:
                avg_reward = np.mean([h['total_reward'] for h in self.training_history[-100:]])
                avg_occupancy = np.mean([h['occupancy_rate'] for h in self.training_history[-100:]])
                print(f"Episode {episode}, Avg Reward: {avg_reward:.2f}, "
                      f"Avg Occupancy: {avg_occupancy:.2f}, Epsilon: {self.agent.epsilon:.3f}")

        print("Training completed!")

    def save_training_results(self, filepath='training_results.json'):
        """Save training history"""
        with open(filepath, 'w') as f:
            json.dump(self.training_history, f, indent=2)

    def plot_training_progress(self):
        """Plot training progress"""
        if not self.training_history:
            print("No training history to plot")
            return

        episodes = [h['episode'] for h in self.training_history]
        rewards = [h['total_reward'] for h in self.training_history]
        occupancy = [h['occupancy_rate'] for h in self.training_history]

        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))

        ax1.plot(episodes, rewards)
        ax1.set_title('Training Rewards')
        ax1.set_xlabel('Episode')
        ax1.set_ylabel('Total Reward')
        ax1.grid(True)

        ax2.plot(episodes, occupancy)
        ax2.set_title('Occupancy Rate')
        ax2.set_xlabel('Episode')
        ax2.set_ylabel('Occupancy Rate')
        ax2.grid(True)

        plt.tight_layout()
        plt.show()

# ================================
# 6. INTERACTIVE SIMULATION
# ================================

class VehiclePlacementSimulator:
    """Interactive simulation interface"""

    def __init__(self, trained_agent=None):
        self.env = VehicleYardEnvironment()
        self.agent = trained_agent
        self.fig, self.ax = plt.subplots(figsize=(12, 8))
        self.animation = None
        self.is_running = False

    def visualize_yard(self, state):
        """Visualize the current yard state"""
        self.ax.clear()

        # Draw grid
        for i in range(self.env.height + 1):
            self.ax.axhline(y=i, color='lightgray', linewidth=0.5)
        for j in range(self.env.width + 1):
            self.ax.axvline(x=j, color='lightgray', linewidth=0.5)

        # Draw roads from zone_map
        for y in range(self.env.height):
            for x in range(self.env.width):
                if self.env.zone_map[y, x] == 0:
                    rect = patches.Rectangle(
                        (x, y), 1, 1,
                        linewidth=0,
                        facecolor='lightgray',  # roads = light gray
                        alpha=0.3
                    )
                    self.ax.add_patch(rect)
                else:
                    rect = patches.Rectangle(
                        (x, y), 1, 1,
                        linewidth=0.2,
                        edgecolor='white',
                        facecolor='none'
                    )
                    self.ax.add_patch(rect)

        # Draw placed vehicles
        colors = plt.cm.Set3(np.linspace(0, 1, len(self.env.placed_vehicles)))
        for i, vehicle in enumerate(self.env.placed_vehicles):
            rect = patches.Rectangle(
                (vehicle['x'], vehicle['y']),
                vehicle['width'], vehicle['height'],
                linewidth=2, edgecolor='black',
                facecolor=colors[i], alpha=0.7
            )
            self.ax.add_patch(rect)

            # Add vehicle type text
            self.ax.text(
                vehicle['x'] + vehicle['width']/2,
                vehicle['y'] + vehicle['height']/2,
                vehicle['type'].upper(),
                ha='center', va='center',
                fontsize=8, fontweight='bold'
            )

        # Draw current vehicle being placed (if any)
        if state['current_vehicle']:
            cv = state['current_vehicle']
            self.ax.text(
                self.env.width + 1, self.env.height - 2,
                f"Next: {cv['type'].upper()}\n{cv['width']}x{cv['height']}",
                fontsize=10, bbox=dict(boxstyle="round,pad=0.3", facecolor="yellow")
            )

        # Add statistics
        stats_text = f"Vehicles Placed: {len(self.env.placed_vehicles)}\n"
        stats_text += f"Occupancy: {state['occupancy_rate']:.1%}\n"
        stats_text += f"Queue: {state['queue_length']}\n"
        stats_text += f"Steps: {state['step_count']}"

        self.ax.text(
            self.env.width + 1, 2,
            stats_text,
            fontsize=10, bbox=dict(boxstyle="round,pad=0.5", facecolor="lightblue")
        )

        self.ax.set_xlim(-0.5, self.env.width + 5)
        self.ax.set_ylim(-0.5, self.env.height + 0.5)
        self.ax.set_aspect('equal')
        self.ax.set_title('Vehicle Placement Simulation', fontsize=14, fontweight='bold')

        plt.draw()

    def run_simulation(self, manual=False):
        """Run the simulation"""
        state = self.env.reset()
        self.visualize_yard(state)

        step = 0
        while state['current_vehicle'] is not None and step < 100:
            if manual:
                # Manual mode - wait for user input
                print(f"\nStep {step}")
                print(f"Current vehicle: {state['current_vehicle']['type']} "
                      f"({state['current_vehicle']['width']}x{state['current_vehicle']['height']})")

                valid_actions = self.env.get_valid_actions()
                print(f"Valid positions: {len(valid_actions)}")

                if valid_actions:
                    action = valid_actions[0]  # Take first valid action for demo
                else:
                    action = 'skip'
            else:
                # AI mode
                if self.agent:
                    valid_actions = self.env.get_valid_actions()
                    action = self.agent.act(state, valid_actions + ['skip'])
                else:
                    # Random baseline
                    valid_actions = self.env.get_valid_actions()
                    action = random.choice(valid_actions) if valid_actions else 'skip'

            state, reward, done, info = self.env.step(action)
            self.visualize_yard(state)

            if done:
                break

            step += 1
            plt.pause(0.5)  # Pause for visualization

        print(f"\nSimulation completed!")
        print(f"Final occupancy rate: {state['occupancy_rate']:.1%}")
        print(f"Vehicles placed: {len(self.env.placed_vehicles)}")

# ================================
# 7. MAIN EXECUTION
# ================================

def main():
    """Main execution function"""
    print("Vehicle Placement RL System")
    print("=" * 40)

    # 1. Generate dataset
    print("1. Generating dataset...")
    generator = VehicleDatasetGenerator()
    dataset = generator.generate_dataset(1000)
    generator.save_dataset(dataset)
    print(f"Generated {len(dataset)} scenarios")

    # 2. Initialize and train model
    print("\n2. Training RL model...")
    trainer = VehiclePlacementTrainer()
    trainer.train(episodes=500)
    trainer.save_training_results()

    # 3. Save trained model
    print("\n3. Saving trained model...")
    trainer.agent.save_model('vehicle_placement_model.pth')

    # 4. Plot training progress
    print("\n4. Plotting training progress...")
    trainer.plot_training_progress()

    # 5. Run simulation
    print("\n5. Running simulation...")
    simulator = VehiclePlacementSimulator(trainer.agent)
    simulator.run_simulation(manual=False)

    print("\nSystem demonstration completed!")

if __name__ == "__main__":
    main()